# -*- coding: utf-8 -*-
"""Estatística descritiva univariada em Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QkNnF5YzIty_WbKma8Rv8gQ39u_QKzjj

## Estatística descritiva univariada 

A estatística descritiva é uma ramo da estatística que visa (que como o nome já diz) para descrever e resumir os dados em medidas que possam ser interpretadas. A necessidade dessas medidas surge por ser, na maioria das vezes, inviável visualizar os dados em sua forma bruta, principalmente hoje em dia em que vivemos num mundo com uma quantidade gigantesca de dados são gerados diariamente.  

Há dois tipos de estatística descritiva: a univariada e a multivariada. A univariada analisa o comportamento isolado de uma variável, enquanto a multivariada analisa mais variáveis e de como elas se relacionam em conjunto. 

E para analisar essas variáveis existem três tipos de medidas, que são:

- **1. Medidas de posição (tendência) central**;

- **2. Medidas de dispersão (ou de variabilidade)**;

- **3. Medidas de assimetria e curtose**;

Todas essas medidas são chamadas de medidas-resumo ou de medidas de sumarização. A seguir irei falar de cada uma dessas medidas e como calculá-las em bibliotecas linguaguem *Python*. Essas bibliotecas são a *Numpy*, *Scipy*, *Statistic*, *Wquantiles* e *Statsmodels*.

### Importando as bibliotecas
"""

pip install wquantiles

import pandas as pd
import numpy as np
import statistics as st
import random
import weighted
import scipy as sc
from scipy.stats import skew, trim_mean, iqr
from scipy.stats.mstats import trimmed_std, trimmed_stde
from scipy.stats.mstats import mquantiles
from scipy.stats import median_absolute_deviation,tvar, variation 
from scipy.stats.mstats import gmean,hmean, trimmed_mean, mode
from scipy.stats import skew, kurtosis
from statsmodels.tools.eval_measures import meanabs
import warnings
warnings.filterwarnings("ignore")

"""### Importando os dados"""

casa = pd.read_csv("Mumbai1.csv")

"""## **Medidas de localização (tendência central ou posição)**

Uma medida de localização mostra um valor em que um conjunto de dados se agrupam em sua volta. As principais medidas de tendência central são a média, mediana e moda. Não irei ater-me apenas nessas três medidas, pos existem outros que são menos conhecidas e que também podem ser usadas na análise descritiva univariada e as bibliotecas citadas anteriormente possuem função que realizam seus cálculo.

Após falar das medidas de tendência central, falarei das demais medidas de localização.

### **Média aritmética**

Basicamente é a soma de todos os valores do conjunto de dados, dividido pelo número de valores. Então a média aritmética de um número $n$ de dados é dada pela função abaixo

$\overline{X} = \dfrac{X_1 + X_2 + \dots + X_n}{n}  = \dfrac{\sum_{i=1}^n X_i}{n}$

Essa medida possui uma limitação: é influenciada por valores extremos ou *outliers*. Esses valores extremos distorcem o valor central gerado no cálculo para a média.

**Calculando a média aritmética em *Python*.**
"""

print('Média com Pandas :',casa['Price'].mean())
print('Média com Numpy:',np.average(casa['Price']))
print('Média com Numpy:',np.mean(casa['Price']))
print('Média com Statistics:',st.mean(casa['Price']))
print('Média com Scipy:',sc.mean(casa['Price']))

"""### **Média geométrica**

Outro tipo de média que é pouco conhecida é a geométrica. Essa média difere da aritmética, pois nessa o valor da média é um produto de todos os valores do conjunto de dados e, em seguida, é aplicada a raiz $n$-ésima como pode-se ver abaixo

$G = \sqrt[n]{X_1 * X_2 * \dots * X_n} = \sqrt[n]{\Pi_{i=1}^n X_i}$

Os valores do conjunto de dados devem ser não-negativos e a média geométrica é menor que a média aritmética.

**Calculando a média geométrica em *Python*.**
"""

#print('Média geométrica com a biblioteca Statistics:',geometric_mean(casa['Price']))
print('Média geométrica com Scipy:',gmean(casa['Price']))

"""### **Média harmônica**

Para a média harmônica os valores do conjunto de dados devem ser diferentes de zero, pois todos os valores serão elevados a -1, somado, divididos pelo número de observações e esses resultado será elevado a -1 novamente, conforme abaixo

$\overline{H} = \bigg(\dfrac{\sum_{i=1}^n X_i^{-1}}{n} \bigg)^{-1}$

A média harmônica é menor que a média geométrica.

**Calculando a média harmônica em *Python*.**
"""

print('Média harmônica com Statistic:',st.harmonic_mean(casa['Price']))
print('Média harmônica com Scipy:',hmean(casa['Price']))

"""### **Média aparada**

Anteriormente, falei que a média pode ser influenciada por valores extremos e que distorce o valor real da média. Uma forma de resolver esse problema é usar a média aparada. Como o nome deixa claro, é uma forma de retirar os valores *outliers* sem mexer do conjunto de dados. Os dados serão ordenados e uma proporção dos valores que estiverem na "ponta" dessa sequência serão desconsideradas no cálculo da média.

$\overline{X}_{aparada} = \dfrac{\sum_{i = p+1}^{n-p} X_{(i)}}{n-2p}$

Onde $p$ é a proporção dos dados desconsiderados.

**Calculando a média aparada em *Python*.**
"""

#aparando 10% dos dados
print('Média 10% aparada com Scipy :',trim_mean(casa['Price'], 0.1))
#aparando 20% dos dados
print('Média 20% aparada com Scipy :',trim_mean(casa['Price'], 0.2))
#aparando 30% dos dados
print('Média 30% aparada com Scipy :',trim_mean(casa['Price'], 0.3))
#aparando 40% dos dados
print('Média 40% aparada com Scipy :',trim_mean(casa['Price'], 0.4))

"""### **Erro padrão da média aparada**

O erro padrão é uma média de variação da média amostral em relação a média populacional.
"""

print('Erro padrão da média aparada com Scipy:',trimmed_stde(casa['Price']))

"""### **Média ponderada**

Existe a média ponderada, onde para cada valor do conjunto de dados é multiplicado por um peso (geralmente um valor entre 0 e 1), os valores da multiplicação são somados e em seguida o valor da soma é dividido pelo somatório dos pesos. Segue a fórmula abaixo

$\overline{X}_{ponderada} = \dfrac{\sum_{i=1}^{n} w_ix_i}{\sum_i^n w_i}$

Essa média é importante, por exemplo, para reduzir a variabilidade dos dados (em dados altamente variáveis os pesos são menores) e/ou para dar corrigir diferenças entre dados de grupos de dados diferentes.
"""

#criando aleatoriamente uma lista de pesos com valores entre 0 e 1 
lista=[]

random.seed(123)
for i in range(6347):
  lista.append(random.random())

"""**Calculando a média ponderada em *Python*.**"""

print("Média ponderada Numpy :",np.average(casa['Price'].tolist(), weights=lista))

"""### **Mediana**

A mediana é o número central em um conjunto de dados. Mediana significa "o valor do meio" quando as observações são ordenadas do menor valor para o maior. Então, existem duas situações:

- Se o número de observações foi ímpar, então a mediana será o valor que está no meio dos dados;

- Se o número de observações for par, toma-se a média dois valores centrais em dois.

$\tilde{X} \left \{ \begin{matrix} \bigg(\dfrac{X_n+1}{2}\bigg), & \mbox{se }n\mbox{ é ímpar} \\ \bigg(\dfrac{X_n}{2}\bigg) + \bigg(\dfrac{X_n+1}{2}\bigg), & \mbox{se }n\mbox{ é par} \end{matrix} \right.$

A medida possui uma vantagem em relação a média : como ela não usa todos os valores do conjunto de dados em seu cálculo, então ela não é influenciada por valores extremos. Por mais aparente uma desvantagem usar a mediana, pelo número de dados usados em seu cálculo, em muitas situações ela é mais vantajosa que a média.

**Calculando a mediana em *Python*.**
"""

print('Mediana com Pandas:',casa['Price'].median())
print('Mediana com Numpy:',np.median(casa['Price']))
print('Mediana com Scipy:',sc.median(casa['Price']))
print('Mediana com Statistics:',st.median(casa['Price']))

"""### **Mediana ponderada**

Não tão conhecida como a medida anterior, essa segue a mesma ideia da média ponderada. Os valores da mediana serão ordenados, multiplicados por um conjunto de pesos, somados e divididos pela soma dos pesos. Em vez de um valor central, a mediana ponderada é um valor cuja a soma dos pesos é o mesmo para as duas metades (inferior e superior) do conjunto de dados.

A mediana ponderada pode ser definida como 

$X_j$ onde  $ j = min_k \bigg[\sum_{i=1}^k w_ix_i  > \dfrac{1}{2} \sum_{i=1}^n w_ix_i \bigg]$

Agora, se 

$ min_k \bigg[\sum_{i=1}^k w_ix_i  = \dfrac{1}{2} \sum_{i=1}^n w_ix_i \bigg]$

A mediana pode ser calculada por $\bigg(\dfrac{X_n}{2}\bigg) + \bigg(\dfrac{X_n+1}{2}\bigg)$.

**Calculando a mediana ponderada em *Python*.**
"""

print('Mediana ponderada com Wquantiles :',weighted.median(casa['Price'], lista))

"""### **Mediana baixa**

Essa medida de mediana é dada em duas situações: Se o número de observações for ímpar, o valor do meio será o valor da mediana; porém, se o valor for par, então a mediana será o menor valor dentre os dois valores intermediários.

$\tilde{X} \left \{ \begin{matrix} \bigg(\dfrac{X_n+1}{2}\bigg), & \mbox{se }n\mbox{ é ímpar} \\ \bigg(\dfrac{X_n}{2}\bigg), & \mbox{se }n\mbox{ é par} \end{matrix} \right.$

**Calculando a baixa mediana com *Python*.**

"""

print('Mediana baixa com Statistics:',st.median_low(casa['Price']))

"""### **Mediana alta**

Essa medida de mediana é dada em duas situações: Se o número de observações for ímpar, o valor do meio será o valor da mediana; porém, se o valor for par, então a mediana será o maior valor dentre os dois valores intermediários.

$\tilde{X} \left \{ \begin{matrix} \bigg(\dfrac{X_n+1}{2}\bigg), & \mbox{se }n\mbox{ é ímpar} \\ \bigg(\dfrac{X_n + 1}{2}\bigg), & \mbox{se }n\mbox{ é par} \end{matrix} \right.$

**Calculando a alta mediana com *Python*.**
"""

print('Mediana alta com Statistics :',st.median_high(casa['Price']))

"""### **Mediana agrupada**

Essa mediana é usada quando os dados contínuos são agrupados em intervalos. Por meio do argumento *interval* é possível alterar os intervalos para calcular a mediana. Não é muito usada, mas fica aqui para fins de curiosidade.

**Calculando a mediana agrupada em *Python*.**

"""

print("Mediana agrupada com Statistics :",st.median_grouped(casa['Price']))
print("Mediana agrupada com Statistics :",st.median_grouped(casa['Price'], interval=2))
print("Mediana agrupada com Statistics :",st.median_grouped(casa['Price'], interval=3))

"""### **Moda**

A moda é o valor que mais se repete em um conjunto de dados. Quando existe apenas um valor que se repete com frequência dizemos que os dados são **unimodais**. Entretanto isso não significa que outros valores não se repitam frequentemente, podem haver outros e dizemos que os dados são ou bimodais, ou trimodais, ou **multimodais**.

**Calculando a moda com *Python*.**

"""

print("Calculando a moda com a biblioteca Pandas :",casa['Price'].mode()[0])
print("Calculando a moda com a biblioteca Statistics :",st.mode(casa['Price']))
print("Calculando com Scipy a Moda é", mode(casa['Price'])[0][0],
      "e o número de vezes que o valor se repete é", mode(casa['Price'])[1][0])

"""**Calculando mais de uma moda com *Python*.**

Com a função *value_counts()* o quanto os valores se repetem. Abaixo temos os 10 preços que mais se repetem.
"""

casa['Price'].value_counts()[0:10]

"""Daqui em diante irei falar sobre as outras medidas de localização que são importantes para a análise univariada.

### **Quantis (Quartis, Decis e Percentis)**

São os valores que dividem o conjunto de dados em partes iguais e que possuem a mesma probabilidade. Para calcular o quantil é necessário, ordenar os dados crescentemente, definir um valor $p$ (que vai dividir a proporção dos dados) e multiplicá-lo pelo número de observações $n$ mais 1

Posição = $p \cdot (n+1)$

encontramos a posição do quantil que dividi os dados para a proporção requerida. Por exemplo, se eu quiser encontrar o quantil que divide os dados ao meio e eu tiver 11 observações basta fazer

Posição = $0.5 \cdot (11+1) = 0.5 \cdot 12 = 6$

Então o valor que está na posição 6 divide os dados na metade.

Geralmente os dados são divididos em 4 partes iguais (quartis), em 10 partes iguais (decis) ou em 100 partes iguais (percentis). 

- Os quartis são denotados por $Q_1$, $Q_2$ e $Q_3$
- Os decis são denotados por $D_1$, $D_2$, $\dots$ , $D_9$;
- Os percentis são denotados por $P_1$, $P_2$, $\dots$ , $P_{99}$;

Bom destacar que o segundo quartil é igual ao quinto decil, que é igual ao quinquagésimo percentil e, por fim, igual a Mediana dos dados.

$Q_2 = D_5 = P_{50} = Mediana$

Então a mediana é um caso particular dessas divisões dos dados.

Criando listas com valores dos quantis.
"""

quartis = list(np.multiply(list(range(1,4)),0.25))
decis = list(np.multiply(list(range(1,9)),0.10))
percentis = list(np.multiply(list(range(1,99)),0.01))

"""**Calculando os Quartis, Decis e Percentis em *Python*.**"""

#Quartis com a biblioteca Pandas
casa['Price'].quantile(quartis)

# Calculando os decis: casa['Price'].quantile(decis)
# Calculando os percentis : casa['Price'].quantile(percentis)

#Quartis com a biblioteca Scipy
mquantiles(casa['Price'],quartis)

# Calculando os decis: mquantiles(casa['Price'],decis)
# Calculando os percentis : mquantiles(casa['Price'],percentis)

#Quartis com a biblioteca Numpy
np.percentile(casa['Price'],np.multiply(quartis,100))

# Calculando os decis: np.percentile(casa['Price'],np.multiply(decis))
# Calculando os percentis : np.percentile(casa['Price'],np.multiply(percentis))

"""## **Medidas de dispersão (ou variabilidade)**

A medidas de posição dão apenas informações parciais sobre os dados e suas distribuições, por isso é necessário analisar como os dados estão variando. Por exemplo, imagine que existam duas maçãs e duas pessoas, uma das pessoas come as duas maçãs, mas média cada uma consumiu uma maçã. Isso não é muito justo e de certa forma não faz sentido.

Como o nome explica, são medida que mostram como os dados estão separados ou compactados, e em que direção eles estão. Em toda ramo da estatística inferencial a variabilidade dos dados é de extrema importância para realizar análises e identificar os melhores modelos.

- Desvios
- Desvio absoluto médio
- Desvio absoluto mediano
- Variância
- Variância aparada
- Desvio padrão
- Desvio padrão aparado
- Amplitude
- Intervalo interquartil
- Coeficiente interquartil

### **Desvio**

Das medidas de dispersão, essa é a mais simples. É a diferença entre do valor de todos dos dados e a média do conjunto de dados. Não é necessário nenhuma análise profunda para perceber que a soma de todos os desvios é igual a zero.

$D = X_i - \overline{X}$ e $\sum_{i=1}^n D = 0$

Essa métrica também pode ser chamada de **erros** ou **resíduos**. Essa medida não é muito importante, mas é a base para desenvolver outras medidas que são relevantes.
"""

casa['Price']-casa['Price'].mean()

"""### **Desvio absoluto médio**

O desvio absoluto médio é a média do do valor absoluto dos desvios da média; também pode ser chamado de **normal l1** ou **norma Manhattan**.
Para calcular essa medida tomamos os desvios, colocamos em valores absolutos, somamos todos e dividimos pelo número de observações. Ao utilizar valores absolutos a soma dos desvios não será zero.

$D.A.M = \dfrac{\sum_{i=1}^n |x_i - \overline{x}|}{n}$
"""

print('Desvio absoluto médio com a biblioteca Pandas :',casa['Price'].mad())
print('Desvio absoluto médio com a biblioteca Statsmodels :',meanabs(casa['Price'],casa['Price'].mean()))

"""### **Desvio absoluto mediano**

O desvio absoluto mediano segue o mesmo raciocínio da mediana, onde calcula-se os desvios, converte os valores para valores absolutos, ordena-se os desvios e encontra-se a mediana na sua forma convencional.

$D.A.Med = mediana(|x_1 - \overline{x}|, |x_2 - \overline{x}|,|x_3 - \overline{x}|,\dots, |x_i - \overline{x}|)$
"""

print('Desvio absoluto mediano com Pandas :',median_absolute_deviation(casa['Price']))

"""### **Variância**

Essa, com certeza, é a mais importante e conhecida das medidas de dispersão. É calculada usando o quadrado dos desvios ou dos resíduos, soma-se todos os valores e dividi-se o resultado pela número de observações menos 1. Segue abaixo a fórmula da variância

$\sigma^2 = \dfrac{\sum_{i=1}^n (X_i - \overline{X})^2}{n-1}$

Essa medida apresenta o incoveniente de apresentar a sua unidade de medida igual ao quadrado da unidade de medida do conjunto de dados. Por exemplo, se os dados estiverem em $m$ a unidade de medida da variância será $m^2$.

**Calculando a variância em *Python*.**

"""

print('Variância com a biblioteca Pandas :',casa['Price'].var())
print('Variância com a biblioteca Statistics :',st.variance(casa['Price']))
print('Variância com a biblioteca Statistics :',st.pvariance(casa['Price']))
print('Variância com a biblioteca Numpy :',np.var(casa['Price']))

"""### **Variância aparada**

Assim como a média, a variância também pode ser influenciada por valores extremos, principalmente porque os valores dos resíduos são elevados ao quadrado, o que faz com que esse valor muito. Assim, sendo o valor pode estar distorcido.

A variância aparada segue a mesma ideia da média aparada: desconsiderar uma proporção de valores que estão "na ponta" dos dados (valores maiores e menores).

**Calculando a variância aparada em *Python*.**
"""

#precisa definir dois valores limites (inferior e superior)
print('Variância aparada com Scipy :',tvar(casa['Price'], limits=[2500000,400000000]))

"""### **Desvio padrão**

Essa medida, assim como a variância, é muito conhecida e bastante usada em testes estatísticos. Pode ser interpretada como a distância média dos dados em relação a média. É a raiz quadrada da variância e resolve o pequeno inconveniente da variância, pois a sua unidade de medida é igual a unidade de medida dos dados; ou seja, se os dados estiverem em $m$ (metros), então o desvio-padrão estará em $m$.

$\sigma = \sqrt{\dfrac{\sum_{i=1}^n (X_i - \overline{X})^2}{n-1}}$

**Calculando o desvio-padrão em *Python*.**

"""

print('Desvio padrão com Numpy :',np.std(casa['Price']))
print('Desvio padrão com Statistics :',st.pstdev(casa['Price']))
print('Desvio padrão com Statistics :',st.stdev(casa['Price']))
print('Desvio padrão com Pandas :',casa['Price'].std())

"""### **Desvio padrão aparado**

Mesma lógica da média aparada e variância parada: desconsiderar valores extremos no cálculo da medida. Com isso podemos ter uma distância média dos dados para a média mais próxima da realidade.

**Calculando o desvio-padrão aparado em *Python*.**
"""

#por padrão apara os dados 10% maiores e 10% menores
print('Desvio padrão aparado com Scipy :',trimmed_std(casa['Price']))

"""### **Amplitude**

A amplitude é a diferença entre os maiores e menores valores observados no conjunto de dados. Essa medida leva em consideração apenas os valores extremos, por esse motivo não é considerada uma boa medida de dispersão.

$Amplitude = X_{máximo} - X_{mínimo}$

**Calculando a amplitude com *Python*.**
"""

print('Amplitude com Numpy :',np.max(casa['Price'])-np.min(casa['Price']))
print('Amplitude com Pandas :',casa['Price'].max() - casa['Price'].min())

"""### **Intervalo interquartil**

O intervalo interquartil ou Amplitude interquartílica é a diferença entre o 75º percentil (3º quartil) e o 25º percentil (1º quartil). Essa medida difere da variância e do desvio padrão, pois ela leva em consideração o ordenamento dos dados. Fórmula abaixo

$IIQ = Q_3 - Q_1$

**Calculando o intervalo interquartil em *Python*.**
"""

print('Intervalo Interquartil com Scipy :',iqr(casa['Price']))

"""### **Coeficiente de variação**

Esse coeficiente mede o grau de dispersão dos dados em torno da média. Uma vantagem dessa medida é que ela é adimensional, um número puro que não é dependente da unidade de medida dos dados; como a variância e o desvio padrão, podendo ser expressa em valores percentuais. 

Para calculá-la toma-se o desvio padrão e dividimos ele pela média dos dados. Caso queira expressá-la em termos percentuais, basta multiplicarmos por 100, conforme abaixo

$CV = \dfrac{\sigma}{\overline{X}} \cdot 100$

Saber se o coeficiente de variação é baixo, médio ou alto vai depender da sua aplicação, porém é uma boa medida para comparar a variabilidade entre duas variáveis.

**Calculando o coeficiente de variação em *Python*.**
"""

print('Coeficiente de variação na Scipy :',variation(casa['Price']))

"""## **Medidas de assimetria e curtose**

Essas medidas estão relacionadas a distribuição dos dados de uma variável. O estudo de distribuições são importantes para realizar a modelagem de populações e existem alguns coeficientes que expressam o comportamento das distribuições.

### **Coeficiente de assimetria**

Essa medida verifica onde os dados estão se concentrando em relação à média. Se os dados estão à direita da média, então há assimetria negativa; se os dados concentram-se a esquerda da média, então há assimetria positiva; e se os dados estiverem concentrados junto a média, dizemos que a distribuição dos dados é simétrica. A fórmula do coeficiente é dada abaixo

$S = \sum_{i=1}^{n} \dfrac{(X_i - \overline{X})^3/n}{\sigma^3}$

Se o valor de $S$ for menor que zero ($S < 0$), a distribuição do conjunto de dados é assimétrica negativa. Se $S > 0$, a distribuição dos dados é assimétrica positiva. Por fim, se $S=0$, então não há assimetria nos dados.

**Calculando o coeficiente de assimetria em *Python*.**
"""

print('Coeficiente de assimetria com Pandas :',casa['Price'].skew())
print('Coeficiente de assimetria com Scipy :',skew(casa['Price']))

"""### **Coeficiente de curtose**

Essa medida diz respeito ao formato da curva de distribuição do conjunto de dados. Se curva é mais plana do que a curva de uma distribuição normal, então diz-se que ela é platicúrtica. Se a curva é mais cônica que a curva de uma distribuição normal, diz-se que ela é leptocúrtica. Se curva é semelhante/igual a curva da distribuição normal, então dizemos que ela é mesocúrtica.

Fórmula do coeficiente de curtose

$S = \sum_{i=1}^{n} \dfrac{(X_i - \overline{X})^4/n}{\sigma^4}$

Interpretação para o valor de $K$

- $K > 3$, então curva leptocúrtica;
- $K = 3$, então curva mesocúrtica;
- $K < 3$, então curva platicúrtica.

**Calculando o coeficiente de curtose em *Python*.**

"""

print('Coeficiente de assimetria com Pandas :',casa['Price'].kurtosis())
print('Coeficiente de assimetria com Scipy :',kurtosis(casa['Price']))

"""Para uma distribuição ser considerada normal o coeficiente de assimetria deve ser igual a zero ($S = 0$) e o coeficiente de curtose igual a três ($K = 3$)."""